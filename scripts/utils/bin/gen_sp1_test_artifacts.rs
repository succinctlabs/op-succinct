use anyhow::Result;
use clap::Parser;
use futures::StreamExt;
use kona_host::HostCli;
use log::info;
use op_succinct_host_utils::{
    block_range::{get_rolling_block_range, get_validated_block_range},
    fetcher::{CacheMode, OPSuccinctDataFetcher, RunContext},
    get_proof_stdin,
    witnessgen::WitnessGenExecutor,
    ProgramType,
};
use rayon::iter::{IndexedParallelIterator, IntoParallelRefIterator, ParallelIterator};
use serde::{Deserialize, Serialize};
use sp1_sdk::{utils, ProverClient, SP1Stdin};
use std::{
    cmp::min,
    fs::{self},
    path::PathBuf,
    time::Duration,
};

pub const MULTI_BLOCK_ELF: &[u8] = include_bytes!("../../../elf/range-elf");

const TWELVE_HOURS: Duration = Duration::from_secs(60 * 60 * 12);

/// Arguments for the `gen-sp1-test-artifacts` executable.
#[derive(Debug, Clone, Parser)]
struct GenSp1TestArtifactsArgs {
    /// The start block of the range to execute.
    #[clap(long)]
    start: Option<u64>,
    /// The end block of the range to execute.
    #[clap(long)]
    end: Option<u64>,
    /// The number of blocks to execute in a single batch.
    #[clap(long, default_value = "300")]
    batch_size: u64,
    /// Use cached witness generation.
    #[clap(long)]
    use_cache: bool,
    /// Use a fixed recent range.
    #[clap(long)]
    rolling: bool,
    /// The environment file to use.
    #[clap(long, default_value = ".env")]
    env_file: PathBuf,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
struct SpanBatchRange {
    start: u64,
    end: u64,
}

/// Split a range of blocks into a list of span batch ranges.
fn split_range(start: u64, end: u64, supplied_range_size: u64) -> Vec<SpanBatchRange> {
    let mut ranges = Vec::new();
    let mut current_start = start;

    while current_start < end {
        let current_end = min(current_start + supplied_range_size, end);
        ranges.push(SpanBatchRange {
            start: current_start,
            end: current_end,
        });
        current_start = current_end;
    }

    ranges
}

/// Concurrently run the native data generation process for each split range.
async fn run_native_data_generation(host_clis: &[HostCli]) {
    const CONCURRENT_NATIVE_HOST_RUNNERS: usize = 5;

    // Split the entire range into chunks of size CONCURRENT_NATIVE_HOST_RUNNERS and process chunks
    // serially. Generate witnesses within each chunk in parallel. This prevents the RPC from
    // being overloaded with too many concurrent requests, while also improving witness generation
    // throughput.
    for chunk in host_clis.chunks(CONCURRENT_NATIVE_HOST_RUNNERS) {
        let mut witnessgen_executor = WitnessGenExecutor::default();

        for host_cli in chunk {
            witnessgen_executor
                .spawn_witnessgen(host_cli)
                .await
                .expect("Failed to spawn witness generation process");
        }

        witnessgen_executor
            .flush()
            .await
            .expect("Failed to generate witnesses");
    }
}

/// Run the zkVM execution process for each split range in parallel. Get the SP1Stdin and the range
/// for each successful execution.
async fn execute_blocks_parallel(
    host_clis: &[HostCli],
    ranges: Vec<SpanBatchRange>,
    prover: &ProverClient,
) -> Vec<(SP1Stdin, SpanBatchRange)> {
    // Run the zkVM execution process for each split range in parallel and fill in the execution stats.
    let successful_ranges = host_clis
        .par_iter()
        .zip(ranges.par_iter())
        .map(|(host_cli, range)| {
            let sp1_stdin = get_proof_stdin(host_cli).unwrap();

            let result = prover.execute(MULTI_BLOCK_ELF, sp1_stdin.clone()).run();

            // If the execution fails, skip this block range and log the error.
            if let Some(err) = result.as_ref().err() {
                log::warn!(
                    "Failed to execute blocks {:?} - {:?} because of {:?}. Reduce your `batch-size` if you're running into OOM issues on SP1.",
                    range.start,
                    range.end,
                    err
                );
                return None;
            }

            Some((sp1_stdin.clone(), range.clone()))
        })
        .filter_map(|result| result)
        .collect();

    info!("Execution is complete.");

    successful_ranges
}

#[tokio::main]
async fn main() -> Result<()> {
    let args = GenSp1TestArtifactsArgs::parse();

    dotenv::from_path(&args.env_file).ok();
    utils::setup_logger();

    let data_fetcher = OPSuccinctDataFetcher::new_with_rollup_config(RunContext::Dev).await?;
    let l2_chain_id = data_fetcher.get_l2_chain_id().await?;

    const COST_ESTIMATOR_ROLLING_RANGE: u64 = 100;
    let (l2_start_block, l2_end_block) = if args.rolling {
        get_rolling_block_range(&data_fetcher, TWELVE_HOURS, COST_ESTIMATOR_ROLLING_RANGE).await?
    } else {
        get_validated_block_range(
            &data_fetcher,
            args.start,
            args.end,
            COST_ESTIMATOR_ROLLING_RANGE,
        )
        .await?
    };

    let split_ranges = split_range(l2_start_block, l2_end_block, args.batch_size);

    info!(
        "The span batch ranges which will be executed: {:?}",
        split_ranges
    );

    let prover = ProverClient::new();

    let cache_mode = if args.use_cache {
        CacheMode::KeepCache
    } else {
        CacheMode::DeleteCache
    };

    // Get the host CLIs in order, in parallel.
    let host_clis = futures::stream::iter(split_ranges.iter())
        .map(|range| async {
            data_fetcher
                .get_host_cli_args(range.start, range.end, ProgramType::Multi, cache_mode)
                .await
                .expect("Failed to get host CLI args")
        })
        .buffered(15)
        .collect::<Vec<_>>()
        .await;

    if !args.use_cache {
        // Get the host CLI args
        run_native_data_generation(&host_clis).await;
    }

    let successful_ranges = execute_blocks_parallel(&host_clis, split_ranges, &prover).await;

    // Now, write the successful ranges to /sp1-testing-suite-artifacts/op-succinct-world-mainnet-{start}-{end}
    // The folders should each have the MULTI_BLOCK_ELF as program.bin, and the serialized stdin should be
    // written to stdin.bin.
    let cargo_metadata = cargo_metadata::MetadataCommand::new().exec().unwrap();
    let root_dir = PathBuf::from(cargo_metadata.workspace_root).join("sp1-testing-suite-artifacts");

    let dir_name = root_dir.join(format!("op-succinct-chain-{}", l2_chain_id));
    info!("Writing artifacts to {:?}", dir_name);
    for (sp1_stdin, range) in successful_ranges {
        let program_dir = PathBuf::from(format!(
            "{}-{}-{}",
            dir_name.to_string_lossy(),
            range.start,
            range.end
        ));
        fs::create_dir_all(&program_dir)?;

        fs::write(program_dir.join("program.bin"), MULTI_BLOCK_ELF)?;
        fs::write(
            program_dir.join("stdin.bin"),
            bincode::serialize(&sp1_stdin).unwrap(),
        )?;
    }

    Ok(())
}
